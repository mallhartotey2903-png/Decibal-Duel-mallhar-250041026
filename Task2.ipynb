{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mallhartotey2903-png/Decibal-Duel-mallhar-250041026/blob/main/Task2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fwXEVqdEzc9f"
      },
      "outputs": [],
      "source": [
        "# ==============================================================================\n",
        "# 0. IMPORTS & INITIAL SETUP\n",
        "# ==============================================================================\n",
        "import os\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchaudio\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import Audio, display\n",
        "import math\n",
        "import json\n",
        "from pathlib import Path\n",
        "torch.manual_seed(42)\n",
        "random.seed(42)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "ctSN1_REzrAQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TrainAudioSpectrogramDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Loads WAVs from category subfolders, returns log-mel-spectrograms\n",
        "    normalized to [-1, 1] and one-hot labels.\n",
        "    \"\"\"\n",
        "    def __init__(self, root_dir, categories, max_frames=512, fraction=1.0, sample_rate=22050):\n",
        "        self.root_dir = Path(root_dir)\n",
        "        self.categories = list(categories)\n",
        "        self.max_frames = max_frames\n",
        "        self.sample_rate = sample_rate\n",
        "        self.file_list = []\n",
        "        self.class_to_idx = {cat: i for i, cat in enumerate(self.categories)}\n",
        "\n",
        "        for cat_name in self.categories:\n",
        "            cat_dir = self.root_dir / cat_name\n",
        "            if not cat_dir.exists() or not cat_dir.is_dir():\n",
        "                continue\n",
        "            files_in_cat = sorted([str(p) for p in cat_dir.glob(\"*.wav\")])\n",
        "            if len(files_in_cat) == 0:\n",
        "                continue\n",
        "            num_to_sample = max(1, int(len(files_in_cat) * fraction))\n",
        "            num_to_sample = min(num_to_sample, len(files_in_cat))\n",
        "            sampled_files = random.sample(files_in_cat, num_to_sample)\n",
        "            label_idx = self.class_to_idx[cat_name]\n",
        "            self.file_list.extend([(file_path, label_idx) for file_path in sampled_files])\n",
        "\n",
        "        if len(self.file_list) == 0:\n",
        "            raise RuntimeError(f\"No wav files found in {root_dir} for categories {categories}\")\n",
        "\n",
        "        # Pre-define mel transform to avoid recreating each call\n",
        "        self.mel_transform = torchaudio.transforms.MelSpectrogram(\n",
        "            sample_rate=self.sample_rate, n_fft=1024, hop_length=256, n_mels=128\n",
        "        )\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.file_list)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        path, label = self.file_list[idx]\n",
        "        wav, sr = torchaudio.load(path)\n",
        "        if sr != self.sample_rate:\n",
        "            # Resample if needed\n",
        "            wav = torchaudio.transforms.Resample(sr, self.sample_rate)(wav)\n",
        "            sr = self.sample_rate\n",
        "\n",
        "        if wav.size(0) > 1:\n",
        "            wav = wav.mean(dim=0, keepdim=True)  # mono\n",
        "\n",
        "        mel_spec = self.mel_transform(wav)  # (1, n_mels, frames)\n",
        "        log_spec = torch.log1p(mel_spec)    # non-negative\n",
        "\n",
        "        # pad / crop in time axis (last dim)\n",
        "        _, _, n_frames = log_spec.shape\n",
        "        if n_frames < self.max_frames:\n",
        "            pad = self.max_frames - n_frames\n",
        "            log_spec = F.pad(log_spec, (0, pad))\n",
        "        else:\n",
        "            log_spec = log_spec[:, :, :self.max_frames]\n",
        "\n",
        "        # Normalize to [-1, 1] using a running heuristic (you can compute dataset stats if desired)\n",
        "        # We use a simple per-sample normalization: divide by (max + eps)\n",
        "        max_val = log_spec.max()\n",
        "        log_spec = log_spec / (max_val + 1e-8)     # now in [0,1]\n",
        "        log_spec = 2 * log_spec - 1                # now in [-1,1]\n",
        "\n",
        "        label_vec = F.one_hot(torch.tensor(label), num_classes=len(self.categories)).float()\n",
        "        return log_spec, label_vec\n",
        "\n",
        "\n",
        "# ==============================================================================\n",
        "# 2. GAN MODEL DEFINITIONS (GENERATOR & DISCRIMINATOR)\n",
        "# ==============================================================================\n",
        "\n",
        "class CGAN_Generator(nn.Module):\n",
        "    def __init__(self, latent_dim, num_categories, spec_shape=(128, 512)):\n",
        "        super().__init__()\n",
        "        self.latent_dim = latent_dim\n",
        "        self.num_categories = num_categories\n",
        "        self.spec_shape = spec_shape\n",
        "\n",
        "        self.fc = nn.Linear(latent_dim + num_categories, 256 * 8 * 32)\n",
        "        self.unflatten_shape = (256, 8, 32)\n",
        "\n",
        "        self.net = nn.Sequential(\n",
        "            nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1), # 16x64\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(inplace=True),\n",
        "\n",
        "            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1),  # 32x128\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(inplace=True),\n",
        "\n",
        "            nn.ConvTranspose2d(64, 32, kernel_size=4, stride=2, padding=1),   # 64x256\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(inplace=True),\n",
        "\n",
        "            nn.ConvTranspose2d(32, 1, kernel_size=4, stride=2, padding=1),    # 128x512\n",
        "            nn.Tanh()  # match the dataset normalization [-1,1]\n",
        "        )\n",
        "\n",
        "    def forward(self, z, y):\n",
        "        # z: (B, latent_dim), y: (B, num_categories)\n",
        "        h = torch.cat([z, y], dim=1)\n",
        "        h = self.fc(h)\n",
        "        h = h.view(-1, *self.unflatten_shape)\n",
        "        fake_spec = self.net(h)  # shape (B, 1, H, W)\n",
        "        return fake_spec\n",
        "\n",
        "class CGAN_Discriminator(nn.Module):\n",
        "    def __init__(self, num_categories, spec_shape=(128, 512), use_spectral_norm=False):\n",
        "        super().__init__()\n",
        "        self.num_categories = num_categories\n",
        "        self.spec_shape = spec_shape\n",
        "        H, W = spec_shape\n",
        "\n",
        "        self.label_embedding = nn.Linear(num_categories, H * W)\n",
        "\n",
        "        conv2d = nn.utils.spectral_norm if use_spectral_norm else (lambda x: x)\n",
        "\n",
        "        self.net = nn.Sequential(\n",
        "            conv2d(nn.Conv2d(2, 32, kernel_size=4, stride=2, padding=1)), # 64x256\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "\n",
        "            conv2d(nn.Conv2d(32, 64, kernel_size=4, stride=2, padding=1)), # 32x128\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "\n",
        "            conv2d(nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1)),# 16x64\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "\n",
        "            conv2d(nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1)),# 8x32\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "\n",
        "            nn.Conv2d(256, 1, kernel_size=(8, 32), stride=1, padding=0) # -> 1x1\n",
        "        )\n",
        "\n",
        "    def forward(self, spec, y):\n",
        "        # spec: (B,1,H,W) in [-1,1]\n",
        "        label_map = self.label_embedding(y).view(-1, 1, *self.spec_shape)\n",
        "        h = torch.cat([spec, label_map], dim=1)\n",
        "        logit = self.net(h)\n",
        "        return logit.view(-1, 1)\n",
        "\n",
        "# ==============================================================================\n",
        "# 3. UTILITY FUNCTIONS (GENERATION, SAVING)\n",
        "# ==============================================================================\n",
        "\n",
        "def generate_audio_gan(generator, category_idx, num_samples, device, sample_rate=22050):\n",
        "    generator.eval()\n",
        "    num_categories = generator.num_categories\n",
        "    latent_dim = generator.latent_dim\n",
        "\n",
        "    # Prepare label and noise (make labels match batch size)\n",
        "    y = F.one_hot(torch.tensor([category_idx]), num_classes=num_categories).float().to(device)\n",
        "    y = y.repeat(num_samples, 1)  # broadcast labels\n",
        "    z = torch.randn(num_samples, latent_dim, device=device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        log_spec_gen = generator(z, y)  # (B,1,H,W) in [-1,1]\n",
        "\n",
        "    # Undo normalization from dataset: [-1,1] -> [0,1]\n",
        "    spec_gen = (log_spec_gen + 1.0) / 2.0\n",
        "\n",
        "    # Convert from 'log1p' domain: we must invert log1p => expm1, but here\n",
        "    # we normalized per-sample using max division; the true inverse is approximate.\n",
        "    # A more faithful pipeline: save generated log-mel (unnormalized) from generator.\n",
        "    # We'll assume generator roughly produces values in scaled-log space:\n",
        "    spec_gen = torch.expm1(spec_gen * 10.0)  # heuristic scale multiplier (tunable)\n",
        "\n",
        "    spec_gen = spec_gen.squeeze(1).to(device)  # (B, n_mels, frames)\n",
        "\n",
        "    inverse_mel = torchaudio.transforms.InverseMelScale(\n",
        "        n_stft=1024 // 2 + 1, n_mels=128, sample_rate=sample_rate\n",
        "    ).to(device)\n",
        "\n",
        "    linear_spec = inverse_mel(spec_gen)  # (B, n_fft_bins, frames)\n",
        "\n",
        "    griffin = torchaudio.transforms.GriffinLim(\n",
        "        n_fft=1024, hop_length=256, win_length=1024, n_iter=32\n",
        "    ).to(device)\n",
        "\n",
        "    waveform = griffin(linear_spec)  # (B, samples)\n",
        "    return waveform.cpu()  # return CPU tensor\n",
        "\n",
        "def save_and_play(wav, sample_rate, filename):\n",
        "    \"\"\"\n",
        "    wav: Tensor of shape (samples,) or (1, samples) or (B, samples) with B=1\n",
        "    \"\"\"\n",
        "    if isinstance(wav, torch.Tensor):\n",
        "        wav_t = wav.detach()\n",
        "    else:\n",
        "        wav_t = torch.tensor(wav)\n",
        "\n",
        "    # If batch dim exists, take first\n",
        "    if wav_t.dim() == 2 and wav_t.size(0) > 1:\n",
        "        wav_t = wav_t[0]\n",
        "\n",
        "    if wav_t.dim() == 2 and wav_t.size(0) == 1:\n",
        "        wav_t = wav_t.squeeze(0)\n",
        "\n",
        "    # Ensure shape (channels, samples) for torchaudio.save; make mono\n",
        "    if wav_t.dim() == 1:\n",
        "        wav_t = wav_t.unsqueeze(0)\n",
        "\n",
        "    # convert to float32\n",
        "    wav_t = wav_t.float()\n",
        "\n",
        "    torchaudio.save(filename, wav_t, sample_rate=sample_rate)\n",
        "    print(f\"Saved to {filename}\")\n",
        "    display(Audio(data=wav_t.numpy(), rate=sample_rate))\n",
        "\n",
        "# ==============================================================================\n",
        "# 4. GAN TRAINING FUNCTION\n",
        "# ==============================================================================\n",
        "\n",
        "def train_gan(generator, discriminator, dataloader, device, categories, epochs, lr, latent_dim):\n",
        "    # Create checkpoint directory at the start of training\n",
        "    os.makedirs(\"checkpoints\", exist_ok=True)\n",
        "\n",
        "    # Optimizers for each model\n",
        "    optimizer_G = torch.optim.Adam(generator.parameters(), lr=lr, betas=(0.5, 0.999))\n",
        "    optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=lr, betas=(0.5, 0.999))\n",
        "\n",
        "    # Loss function\n",
        "    criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "    # Create directories for output\n",
        "    os.makedirs(\"gan_generated_audio\", exist_ok=True)\n",
        "    os.makedirs(\"gan_spectrogram_plots\", exist_ok=True)\n",
        "\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        loop = tqdm(dataloader, desc=f\"Epoch {epoch}/{epochs}\", leave=True)\n",
        "        for real_specs, labels in loop:\n",
        "            real_specs = real_specs.to(device)\n",
        "            labels = labels.to(device)\n",
        "            batch_size = real_specs.size(0)\n",
        "\n",
        "            # Labels for loss calculation\n",
        "            real_labels_tensor = torch.ones(batch_size, 1, device=device)\n",
        "            fake_labels_tensor = torch.zeros(batch_size, 1, device=device)\n",
        "\n",
        "            # ---------------------\n",
        "            #  Train Discriminator\n",
        "            # ---------------------\n",
        "            optimizer_D.zero_grad()\n",
        "            real_output = discriminator(real_specs, labels)\n",
        "            loss_D_real = criterion(real_output, real_labels_tensor)\n",
        "\n",
        "            z = torch.randn(batch_size, latent_dim, device=device)\n",
        "            fake_specs = generator(z, labels)\n",
        "\n",
        "            fake_output = discriminator(fake_specs.detach(), labels)\n",
        "            loss_D_fake = criterion(fake_output, fake_labels_tensor)\n",
        "\n",
        "            loss_D = loss_D_real + loss_D_fake\n",
        "            loss_D.backward()\n",
        "            optimizer_D.step()\n",
        "\n",
        "            # -----------------\n",
        "            #  Train Generator\n",
        "            # -----------------\n",
        "            optimizer_G.zero_grad()\n",
        "            output = discriminator(fake_specs, labels)\n",
        "            loss_G = criterion(output, real_labels_tensor)\n",
        "\n",
        "            loss_G.backward()\n",
        "            optimizer_G.step()\n",
        "\n",
        "            loop.set_postfix(loss_D=loss_D.item(), loss_G=loss_G.item())\n",
        "\n",
        "        # --- End of Epoch: Generate and save samples ---\n",
        "        if epoch % 20 == 0:\n",
        "            print(f\"\\n--- Generating Samples for Epoch {epoch} ---\")\n",
        "            generator.eval()\n",
        "\n",
        "            fig, axes = plt.subplots(1, len(categories), figsize=(4 * len(categories), 4))\n",
        "            if len(categories) == 1:\n",
        "                axes = [axes]\n",
        "\n",
        "            for cat_idx, cat_name in enumerate(categories):\n",
        "                y_cond = F.one_hot(torch.tensor([cat_idx]), num_classes=generator.num_categories).float().to(device)\n",
        "                z_sample = torch.randn(1, generator.latent_dim).to(device)\n",
        "                with torch.no_grad():\n",
        "                    spec_gen_log = generator(z_sample, y_cond)\n",
        "\n",
        "                spec_gen_log_np = spec_gen_log.squeeze().cpu().numpy()\n",
        "                axes[cat_idx].imshow(spec_gen_log_np, aspect='auto', origin='lower', cmap='viridis')\n",
        "                axes[cat_idx].set_title(f'{cat_name} (Epoch {epoch})')\n",
        "                axes[cat_idx].axis('off')\n",
        "\n",
        "            plt.tight_layout()\n",
        "            plt.savefig(f'gan_spectrogram_plots/epoch_{epoch:03d}.png')\n",
        "            plt.show()\n",
        "            plt.close(fig)\n",
        "\n",
        "            for cat_idx, cat_name in enumerate(categories):\n",
        "                wav = generate_audio_gan(generator, cat_idx, 1, device)\n",
        "                fname = f\"gan_generated_audio/{cat_name}_ep{epoch}.wav\"\n",
        "                save_and_play(wav, sample_rate=22050, filename=fname)\n",
        "\n",
        "            generator.train()\n",
        "            print(\"--- End of Sample Generation ---\\n\")\n",
        "\n",
        "        # -------------------------------\n",
        "        # Save checkpoint for this epoch\n",
        "        # -------------------------------\n",
        "        torch.save({\n",
        "            'epoch': epoch,\n",
        "            'generator_state': generator.state_dict(),\n",
        "            'discriminator_state': discriminator.state_dict(),\n",
        "            'optG_state': optimizer_G.state_dict(),\n",
        "            'optD_state': optimizer_D.state_dict()\n",
        "        }, f'checkpoints/gan_epoch_{epoch:03d}.pt')\n"
      ],
      "metadata": {
        "id": "owRE7CP3zv7D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===============================================================\n",
        "# 5. MAIN EXECUTION BLOCK\n",
        "# ==============================================================================\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # --- Configuration ---\n",
        "    DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    LATENT_DIM = 100 # Standard for GANs\n",
        "    EPOCHS = 100 # GANs often require more epochs\n",
        "    BATCH_SIZE = 8\n",
        "    LEARNING_RATE = 2e-4 # Common learning rate for GANs with Adam\n",
        "\n",
        "    # --- Paths and Data Setup ---\n",
        "    BASE_PATH = '/content/drive/MyDrive/organized_dataset'\n",
        "    TRAIN_PATH = os.path.join(BASE_PATH, 'train')\n",
        "    train_categories = sorted([d for d in os.listdir(TRAIN_PATH) if os.path.isdir(os.path.join(TRAIN_PATH, d))])\n",
        "    NUM_CATEGORIES = len(train_categories)\n",
        "\n",
        "    print(f\"Using device: {DEVICE}\")\n",
        "    print(f\"Found {NUM_CATEGORIES} categories: {train_categories}\")\n",
        "\n",
        "    train_dataset = TrainAudioSpectrogramDataset(\n",
        "        root_dir=TRAIN_PATH, categories=train_categories\n",
        "    )\n",
        "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n",
        "\n",
        "    # --- Initialize Models ---\n",
        "    generator = CGAN_Generator(LATENT_DIM, NUM_CATEGORIES).to(DEVICE)\n",
        "    discriminator = CGAN_Discriminator(NUM_CATEGORIES).to(DEVICE)\n",
        "\n",
        "    # --- Start Training ---\n",
        "    train_gan(\n",
        "        generator=generator,\n",
        "        discriminator=discriminator,\n",
        "        dataloader=train_loader,\n",
        "        device=DEVICE,\n",
        "        categories=train_categories,\n",
        "        epochs=EPOCHS,\n",
        "        lr=LEARNING_RATE,\n",
        "        latent_dim=LATENT_DIM\n",
        "    )"
      ],
      "metadata": {
        "id": "QhfBSly6zzrc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3dAYmgsh1QNz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}