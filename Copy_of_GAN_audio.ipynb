{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mallhartotey2903-png/Decibal-Duel-mallhar-250041026/blob/main/Copy_of_GAN_audio.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch torchaudio torchvision transformers"
      ],
      "metadata": {
        "id": "nt7pXqzaznaz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "674791f3-6d34-4b94-810d-6612eb08e03f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (0.23.0+cu126)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision) (11.3.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.36.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.6.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2025.10.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fwXEVqdEzc9f"
      },
      "outputs": [],
      "source": [
        "# ==============================================================================\n",
        "# 0. IMPORTS & INITIAL SETUP\n",
        "# ==============================================================================\n",
        "import os\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchaudio\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import Audio, display\n",
        "import math\n",
        "import json\n",
        "from pathlib import Path\n",
        "torch.manual_seed(42)\n",
        "random.seed(42)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ctSN1_REzrAQ",
        "outputId": "5dd6163a-ed0b-4f66-ad26-9afad91d8e6a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class TrainAudioSpectrogramDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Loads WAVs from category subfolders, returns log-mel-spectrograms\n",
        "    normalized to [-1, 1] and one-hot labels.\n",
        "    \"\"\"\n",
        "    def __init__(self, root_dir, categories, max_frames=512, fraction=1.0, sample_rate=22050):\n",
        "        self.root_dir = Path(root_dir)\n",
        "        self.categories = list(categories)\n",
        "        self.max_frames = max_frames\n",
        "        self.sample_rate = sample_rate\n",
        "        self.file_list = []\n",
        "        self.class_to_idx = {cat: i for i, cat in enumerate(self.categories)}\n",
        "\n",
        "        for cat_name in self.categories:\n",
        "            cat_dir = self.root_dir / cat_name\n",
        "            if not cat_dir.exists() or not cat_dir.is_dir():\n",
        "                continue\n",
        "            files_in_cat = sorted([str(p) for p in cat_dir.glob(\"*.wav\")])\n",
        "            if len(files_in_cat) == 0:\n",
        "                continue\n",
        "            num_to_sample = max(1, int(len(files_in_cat) * fraction))\n",
        "            num_to_sample = min(num_to_sample, len(files_in_cat))\n",
        "            sampled_files = random.sample(files_in_cat, num_to_sample)\n",
        "            label_idx = self.class_to_idx[cat_name]\n",
        "            self.file_list.extend([(file_path, label_idx) for file_path in sampled_files])\n",
        "\n",
        "        if len(self.file_list) == 0:\n",
        "            raise RuntimeError(f\"No wav files found in {root_dir} for categories {categories}\")\n",
        "\n",
        "        # Pre-define mel transform to avoid recreating each call\n",
        "        self.mel_transform = torchaudio.transforms.MelSpectrogram(\n",
        "            sample_rate=self.sample_rate, n_fft=1024, hop_length=256, n_mels=128\n",
        "        )\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.file_list)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        path, label = self.file_list[idx]\n",
        "        wav, sr = torchaudio.load(path)\n",
        "        if sr != self.sample_rate:\n",
        "            # Resample if needed\n",
        "            wav = torchaudio.transforms.Resample(sr, self.sample_rate)(wav)\n",
        "            sr = self.sample_rate\n",
        "\n",
        "        if wav.size(0) > 1:\n",
        "            wav = wav.mean(dim=0, keepdim=True)  # mono\n",
        "\n",
        "        mel_spec = self.mel_transform(wav)  # (1, n_mels, frames)\n",
        "        log_spec = torch.log1p(mel_spec)    # non-negative\n",
        "\n",
        "        # pad / crop in time axis (last dim)\n",
        "        _, _, n_frames = log_spec.shape\n",
        "        if n_frames < self.max_frames:\n",
        "            pad = self.max_frames - n_frames\n",
        "            log_spec = F.pad(log_spec, (0, pad))\n",
        "        else:\n",
        "            log_spec = log_spec[:, :, :self.max_frames]\n",
        "\n",
        "        # Normalize to [-1, 1] using a running heuristic (you can compute dataset stats if desired)\n",
        "        # We use a simple per-sample normalization: divide by (max + eps)\n",
        "        max_val = log_spec.max()\n",
        "        log_spec = log_spec / (max_val + 1e-8)     # now in [0,1]\n",
        "        log_spec = 2 * log_spec - 1                # now in [-1,1]\n",
        "\n",
        "        label_vec = F.one_hot(torch.tensor(label), num_classes=len(self.categories)).float()\n",
        "        return log_spec, label_vec\n",
        "\n",
        "\n",
        "# ==============================================================================\n",
        "# 2. GAN MODEL DEFINITIONS (GENERATOR & DISCRIMINATOR)\n",
        "# ==============================================================================\n",
        "\n",
        "class CGAN_Generator(nn.Module):\n",
        "    def __init__(self, latent_dim, num_categories, spec_shape=(128, 512)):\n",
        "        super().__init__()\n",
        "        self.latent_dim = latent_dim\n",
        "        self.num_categories = num_categories\n",
        "        self.spec_shape = spec_shape\n",
        "\n",
        "        self.fc = nn.Linear(latent_dim + num_categories, 256 * 8 * 32)\n",
        "        self.unflatten_shape = (256, 8, 32)\n",
        "\n",
        "        self.net = nn.Sequential(\n",
        "            nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1), # 16x64\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(inplace=True),\n",
        "\n",
        "            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1),  # 32x128\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(inplace=True),\n",
        "\n",
        "            nn.ConvTranspose2d(64, 32, kernel_size=4, stride=2, padding=1),   # 64x256\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(inplace=True),\n",
        "\n",
        "            nn.ConvTranspose2d(32, 1, kernel_size=4, stride=2, padding=1),    # 128x512\n",
        "            nn.Tanh()  # match the dataset normalization [-1,1]\n",
        "        )\n",
        "\n",
        "    def forward(self, z, y):\n",
        "        # z: (B, latent_dim), y: (B, num_categories)\n",
        "        h = torch.cat([z, y], dim=1)\n",
        "        h = self.fc(h)\n",
        "        h = h.view(-1, *self.unflatten_shape)\n",
        "        fake_spec = self.net(h)  # shape (B, 1, H, W)\n",
        "        return fake_spec\n",
        "\n",
        "class CGAN_Discriminator(nn.Module):\n",
        "    def __init__(self, num_categories, spec_shape=(128, 512), use_spectral_norm=False):\n",
        "        super().__init__()\n",
        "        self.num_categories = num_categories\n",
        "        self.spec_shape = spec_shape\n",
        "        H, W = spec_shape\n",
        "\n",
        "        self.label_embedding = nn.Linear(num_categories, H * W)\n",
        "\n",
        "        conv2d = nn.utils.spectral_norm if use_spectral_norm else (lambda x: x)\n",
        "\n",
        "        self.net = nn.Sequential(\n",
        "            conv2d(nn.Conv2d(2, 32, kernel_size=4, stride=2, padding=1)), # 64x256\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "\n",
        "            conv2d(nn.Conv2d(32, 64, kernel_size=4, stride=2, padding=1)), # 32x128\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "\n",
        "            conv2d(nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1)),# 16x64\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "\n",
        "            conv2d(nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1)),# 8x32\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "\n",
        "            nn.Conv2d(256, 1, kernel_size=(8, 32), stride=1, padding=0) # -> 1x1\n",
        "        )\n",
        "\n",
        "    def forward(self, spec, y):\n",
        "        # spec: (B,1,H,W) in [-1,1]\n",
        "        label_map = self.label_embedding(y).view(-1, 1, *self.spec_shape)\n",
        "        h = torch.cat([spec, label_map], dim=1)\n",
        "        logit = self.net(h)\n",
        "        return logit.view(-1, 1)\n",
        "\n",
        "# ==============================================================================\n",
        "# 3. UTILITY FUNCTIONS (GENERATION, SAVING)\n",
        "# ==============================================================================\n",
        "\n",
        "def generate_audio_gan(generator, category_idx, num_samples, device, sample_rate=22050):\n",
        "    generator.eval()\n",
        "    num_categories = generator.num_categories\n",
        "    latent_dim = generator.latent_dim\n",
        "\n",
        "    # Prepare label and noise (make labels match batch size)\n",
        "    y = F.one_hot(torch.tensor([category_idx]), num_classes=num_categories).float().to(device)\n",
        "    y = y.repeat(num_samples, 1)  # broadcast labels\n",
        "    z = torch.randn(num_samples, latent_dim, device=device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        log_spec_gen = generator(z, y)  # (B,1,H,W) in [-1,1]\n",
        "\n",
        "    # Undo normalization from dataset: [-1,1] -> [0,1]\n",
        "    spec_gen = (log_spec_gen + 1.0) / 2.0\n",
        "\n",
        "    # Convert from 'log1p' domain: we must invert log1p => expm1, but here\n",
        "    # we normalized per-sample using max division; the true inverse is approximate.\n",
        "    # A more faithful pipeline: save generated log-mel (unnormalized) from generator.\n",
        "    # We'll assume generator roughly produces values in scaled-log space:\n",
        "    spec_gen = torch.expm1(spec_gen * 10.0)  # heuristic scale multiplier (tunable)\n",
        "\n",
        "    spec_gen = spec_gen.squeeze(1).to(device)  # (B, n_mels, frames)\n",
        "\n",
        "    inverse_mel = torchaudio.transforms.InverseMelScale(\n",
        "        n_stft=1024 // 2 + 1, n_mels=128, sample_rate=sample_rate\n",
        "    ).to(device)\n",
        "\n",
        "    linear_spec = inverse_mel(spec_gen)  # (B, n_fft_bins, frames)\n",
        "\n",
        "    griffin = torchaudio.transforms.GriffinLim(\n",
        "        n_fft=1024, hop_length=256, win_length=1024, n_iter=32\n",
        "    ).to(device)\n",
        "\n",
        "    waveform = griffin(linear_spec)  # (B, samples)\n",
        "    return waveform.cpu()  # return CPU tensor\n",
        "\n",
        "def save_and_play(wav, sample_rate, filename):\n",
        "    \"\"\"\n",
        "    wav: Tensor of shape (samples,) or (1, samples) or (B, samples) with B=1\n",
        "    \"\"\"\n",
        "    if isinstance(wav, torch.Tensor):\n",
        "        wav_t = wav.detach()\n",
        "    else:\n",
        "        wav_t = torch.tensor(wav)\n",
        "\n",
        "    # If batch dim exists, take first\n",
        "    if wav_t.dim() == 2 and wav_t.size(0) > 1:\n",
        "        wav_t = wav_t[0]\n",
        "\n",
        "    if wav_t.dim() == 2 and wav_t.size(0) == 1:\n",
        "        wav_t = wav_t.squeeze(0)\n",
        "\n",
        "    # Ensure shape (channels, samples) for torchaudio.save; make mono\n",
        "    if wav_t.dim() == 1:\n",
        "        wav_t = wav_t.unsqueeze(0)\n",
        "\n",
        "    # convert to float32\n",
        "    wav_t = wav_t.float()\n",
        "\n",
        "    torchaudio.save(filename, wav_t, sample_rate=sample_rate)\n",
        "    print(f\"Saved to {filename}\")\n",
        "    display(Audio(data=wav_t.numpy(), rate=sample_rate))\n",
        "\n",
        "# ==============================================================================\n",
        "# 4. GAN TRAINING FUNCTION\n",
        "# ==============================================================================\n",
        "\n",
        "def train_gan(generator, discriminator, dataloader, device, categories, epochs, lr, latent_dim):\n",
        "    # Create checkpoint directory at the start of training\n",
        "    os.makedirs(\"checkpoints\", exist_ok=True)\n",
        "\n",
        "    # Optimizers for each model\n",
        "    optimizer_G = torch.optim.Adam(generator.parameters(), lr=lr, betas=(0.5, 0.999))\n",
        "    optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=lr, betas=(0.5, 0.999))\n",
        "\n",
        "    # Loss function\n",
        "    criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "    # Create directories for output\n",
        "    os.makedirs(\"gan_generated_audio\", exist_ok=True)\n",
        "    os.makedirs(\"gan_spectrogram_plots\", exist_ok=True)\n",
        "\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        loop = tqdm(dataloader, desc=f\"Epoch {epoch}/{epochs}\", leave=True)\n",
        "        for real_specs, labels in loop:\n",
        "            real_specs = real_specs.to(device)\n",
        "            labels = labels.to(device)\n",
        "            batch_size = real_specs.size(0)\n",
        "\n",
        "            # Labels for loss calculation\n",
        "            real_labels_tensor = torch.ones(batch_size, 1, device=device)\n",
        "            fake_labels_tensor = torch.zeros(batch_size, 1, device=device)\n",
        "\n",
        "            # ---------------------\n",
        "            #  Train Discriminator\n",
        "            # ---------------------\n",
        "            optimizer_D.zero_grad()\n",
        "            real_output = discriminator(real_specs, labels)\n",
        "            loss_D_real = criterion(real_output, real_labels_tensor)\n",
        "\n",
        "            z = torch.randn(batch_size, latent_dim, device=device)\n",
        "            fake_specs = generator(z, labels)\n",
        "\n",
        "            fake_output = discriminator(fake_specs.detach(), labels)\n",
        "            loss_D_fake = criterion(fake_output, fake_labels_tensor)\n",
        "\n",
        "            loss_D = loss_D_real + loss_D_fake\n",
        "            loss_D.backward()\n",
        "            optimizer_D.step()\n",
        "\n",
        "            # -----------------\n",
        "            #  Train Generator\n",
        "            # -----------------\n",
        "            optimizer_G.zero_grad()\n",
        "            output = discriminator(fake_specs, labels)\n",
        "            loss_G = criterion(output, real_labels_tensor)\n",
        "\n",
        "            loss_G.backward()\n",
        "            optimizer_G.step()\n",
        "\n",
        "            loop.set_postfix(loss_D=loss_D.item(), loss_G=loss_G.item())\n",
        "\n",
        "        # --- End of Epoch: Generate and save samples ---\n",
        "        if epoch % 1 == 0:\n",
        "            print(f\"\\n--- Generating Samples for Epoch {epoch} ---\")\n",
        "            generator.eval()\n",
        "\n",
        "            fig, axes = plt.subplots(1, len(categories), figsize=(4 * len(categories), 4))\n",
        "            if len(categories) == 1:\n",
        "                axes = [axes]\n",
        "\n",
        "            for cat_idx, cat_name in enumerate(categories):\n",
        "                y_cond = F.one_hot(torch.tensor([cat_idx]), num_classes=generator.num_categories).float().to(device)\n",
        "                z_sample = torch.randn(1, generator.latent_dim).to(device)\n",
        "                with torch.no_grad():\n",
        "                    spec_gen_log = generator(z_sample, y_cond)\n",
        "\n",
        "                spec_gen_log_np = spec_gen_log.squeeze().cpu().numpy()\n",
        "                axes[cat_idx].imshow(spec_gen_log_np, aspect='auto', origin='lower', cmap='viridis')\n",
        "                axes[cat_idx].set_title(f'{cat_name} (Epoch {epoch})')\n",
        "                axes[cat_idx].axis('off')\n",
        "\n",
        "            plt.tight_layout()\n",
        "            plt.savefig(f'gan_spectrogram_plots/epoch_{epoch:03d}.png')\n",
        "            plt.show()\n",
        "            plt.close(fig)\n",
        "\n",
        "            for cat_idx, cat_name in enumerate(categories):\n",
        "                wav = generate_audio_gan(generator, cat_idx, 1, device)\n",
        "                fname = f\"gan_generated_audio/{cat_name}_ep{epoch}.wav\"\n",
        "                save_and_play(wav, sample_rate=22050, filename=fname)\n",
        "\n",
        "            generator.train()\n",
        "            print(\"--- End of Sample Generation ---\\n\")\n",
        "\n",
        "        # -------------------------------\n",
        "        # Save checkpoint for this epoch\n",
        "        # -------------------------------\n",
        "        torch.save({\n",
        "            'epoch': epoch,\n",
        "            'generator_state': generator.state_dict(),\n",
        "            'discriminator_state': discriminator.state_dict(),\n",
        "            'optG_state': optimizer_G.state_dict(),\n",
        "            'optD_state': optimizer_D.state_dict()\n",
        "        }, f'checkpoints/gan_epoch_{epoch:03d}.pt')\n"
      ],
      "metadata": {
        "id": "owRE7CP3zv7D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===============================================================\n",
        "# 5. MAIN EXECUTION BLOCK\n",
        "# ==============================================================================\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # --- Configuration ---\n",
        "    DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    LATENT_DIM = 100 # Standard for GANs\n",
        "    EPOCHS = 200 # GANs often require more epochs\n",
        "    BATCH_SIZE = 32\n",
        "    LEARNING_RATE = 2e-4 # Common learning rate for GANs with Adam\n",
        "\n",
        "    # --- Paths and Data Setup ---\n",
        "    BASE_PATH = '/content/drive/MyDrive/organized_dataset'\n",
        "    TRAIN_PATH = os.path.join(BASE_PATH, 'train')\n",
        "    train_categories = sorted([d for d in os.listdir(TRAIN_PATH) if os.path.isdir(os.path.join(TRAIN_PATH, d))])\n",
        "    NUM_CATEGORIES = len(train_categories)\n",
        "\n",
        "    print(f\"Using device: {DEVICE}\")\n",
        "    print(f\"Found {NUM_CATEGORIES} categories: {train_categories}\")\n",
        "\n",
        "    train_dataset = TrainAudioSpectrogramDataset(\n",
        "        root_dir=TRAIN_PATH, categories=train_categories\n",
        "    )\n",
        "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n",
        "\n",
        "    # --- Initialize Models ---\n",
        "    generator = CGAN_Generator(LATENT_DIM, NUM_CATEGORIES).to(DEVICE)\n",
        "    discriminator = CGAN_Discriminator(NUM_CATEGORIES).to(DEVICE)\n",
        "\n",
        "    # --- Start Training ---\n",
        "    train_gan(\n",
        "        generator=generator,\n",
        "        discriminator=discriminator,\n",
        "        dataloader=train_loader,\n",
        "        device=DEVICE,\n",
        "        categories=train_categories,\n",
        "        epochs=EPOCHS,\n",
        "        lr=LEARNING_RATE,\n",
        "        latent_dim=LATENT_DIM\n",
        "    )"
      ],
      "metadata": {
        "id": "QhfBSly6zzrc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 497
        },
        "outputId": "5baa89e9-daaf-40a3-adda-86fb85512f82"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Found 5 categories: ['dog_bark', 'drilling', 'engine_idling', 'siren', 'street_music']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch 1/200:   0%|          | 0/108 [00:00<?, ?it/s]/usr/local/lib/python3.12/dist-packages/torchaudio/_backend/utils.py:213: UserWarning: In 2.9, this function's implementation will be changed to use torchaudio.load_with_torchcodec` under the hood. Some parameters like ``normalize``, ``format``, ``buffer_size``, and ``backend`` will be ignored. We recommend that you port your code to rely directly on TorchCodec's decoder instead: https://docs.pytorch.org/torchcodec/stable/generated/torchcodec.decoders.AudioDecoder.html#torchcodec.decoders.AudioDecoder.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torchaudio/_backend/ffmpeg.py:88: UserWarning: torio.io._streaming_media_decoder.StreamingMediaDecoder has been deprecated. This deprecation is part of a large refactoring effort to transition TorchAudio into a maintenance phase. The decoding and encoding capabilities of PyTorch for both audio and video are being consolidated into TorchCodec. Please see https://github.com/pytorch/audio/issues/3902 for more information. It will be removed from the 2.9 release. \n",
            "  s = torchaudio.io.StreamReader(src, format, None, buffer_size)\n",
            "Epoch 1/200:  97%|█████████▋| 105/108 [35:50<01:01, 20.48s/it, loss_D=0.0026, loss_G=7.32]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3814045323.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;31m# --- Start Training ---\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m     train_gan(\n\u001b[0m\u001b[1;32m     33\u001b[0m         \u001b[0mgenerator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mdiscriminator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdiscriminator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-1385647502.py\u001b[0m in \u001b[0;36mtrain_gan\u001b[0;34m(generator, discriminator, dataloader, device, categories, epochs, lr, latent_dim)\u001b[0m\n\u001b[1;32m    233\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m         \u001b[0mloop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mf\"Epoch {epoch}/{epochs}\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mleave\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 235\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mreal_specs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mloop\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    236\u001b[0m             \u001b[0mreal_specs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreal_specs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m             \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1180\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1181\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1182\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1183\u001b[0m                 \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    732\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    733\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 734\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    735\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    736\u001b[0m             if (\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    788\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    789\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 790\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    791\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    792\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-1385647502.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfile_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m         \u001b[0mwav\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorchaudio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msr\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_rate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m             \u001b[0;31m# Resample if needed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torchaudio/_backend/utils.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(uri, frame_offset, num_frames, normalize, channels_first, format, buffer_size, backend)\u001b[0m\n\u001b[1;32m    220\u001b[0m         )\n\u001b[1;32m    221\u001b[0m         \u001b[0mbackend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdispatcher\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muri\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbackend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 222\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muri\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframe_offset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_frames\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchannels_first\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    223\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torchaudio/_backend/ffmpeg.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(uri, frame_offset, num_frames, normalize, channels_first, format, buffer_size)\u001b[0m\n\u001b[1;32m    295\u001b[0m         \u001b[0mbuffer_size\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m4096\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m     ) -> Tuple[torch.Tensor, int]:\n\u001b[0;32m--> 297\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mload_audio\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muri\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframe_offset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_frames\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchannels_first\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    298\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torchaudio/_backend/ffmpeg.py\u001b[0m in \u001b[0;36mload_audio\u001b[0;34m(src, frame_offset, num_frames, convert, channels_first, format, buffer_size)\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"read\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mformat\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"vorbis\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0mformat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"ogg\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m     \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorchaudio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStreamReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m     \u001b[0msample_rate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_src_stream_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefault_audio_stream\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0mfilter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_load_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe_offset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_frames\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torchaudio/_internal/module_utils.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0mmessage\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34mf' It will be removed from {\"a future\" if version is None else \"the \" + str(version)} release. '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torio/io/_streaming_media_decoder.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, format, option, buffer_size)\u001b[0m\n\u001b[1;32m    523\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_be\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mffmpeg_ext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStreamingMediaDecoderFileObj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moption\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    524\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 525\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_be\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mffmpeg_ext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStreamingMediaDecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormpath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moption\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    526\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    527\u001b[0m         \u001b[0mi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_be\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_best_audio_stream\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3dAYmgsh1QNz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}